{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from evaluate import evaluator\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "from peft import PeftConfig, PeftModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T03:37:05.084521Z",
     "start_time": "2024-03-28T03:37:01.379175Z"
    }
   },
   "id": "3e65a5376f83b774",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T03:37:05.100525Z",
     "start_time": "2024-03-28T03:37:05.085522Z"
    }
   },
   "id": "11f772f6032445c4",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the model. Using pretrained model with quantization. Test for it's performance."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e91e5a09a1415cf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_weight=True,\n",
    "    bnb_4bit_activation=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T03:37:05.116529Z",
     "start_time": "2024-03-28T03:37:05.101525Z"
    }
   },
   "id": "c0636331122d3b70",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "models = [\n",
    "    'peft/bert-base-uncased',\n",
    "    'peft/bert-large-uncased',\n",
    "    'peft/roberta-base',\n",
    "    'peft/roberta-large',\n",
    "    'peft/distilbert-base-uncased',\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T03:37:05.132533Z",
     "start_time": "2024-03-28T03:37:05.117529Z"
    }
   },
   "id": "16850ce70ad2f69a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/bert-base-uncased' eval_results={'accuracy': 0.921, 'recall': 0.9262295081967213, 'precision': 0.9131313131313131, 'f1': 0.9196337741607324, 'total_time_in_seconds': 263.3712404000107, 'samples_per_second': 3.7969217841750322, 'latency_in_seconds': 0.2633712404000107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/bert-large-uncased' eval_results={'accuracy': 0.923, 'recall': 0.9200819672131147, 'precision': 0.9219712525667351, 'f1': 0.921025641025641, 'total_time_in_seconds': 910.1546687999507, 'samples_per_second': 1.0987143551309926, 'latency_in_seconds': 0.9101546687999508}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/roberta-base' eval_results={'accuracy': 0.937, 'recall': 0.9139344262295082, 'precision': 0.9550321199143469, 'f1': 0.9340314136125655, 'total_time_in_seconds': 259.067527599982, 'samples_per_second': 3.859997465772972, 'latency_in_seconds': 0.259067527599982}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/roberta-large' eval_results={'accuracy': 0.965, 'recall': 0.9487704918032787, 'precision': 0.9788583509513742, 'f1': 0.963579604578564, 'total_time_in_seconds': 885.460542100016, 'samples_per_second': 1.1293558012515552, 'latency_in_seconds': 0.885460542100016}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/distilbert-base-uncased' eval_results={'accuracy': 0.861, 'recall': 0.8442622950819673, 'precision': 0.8673684210526316, 'f1': 0.8556593977154725, 'total_time_in_seconds': 130.0178044999484, 'samples_per_second': 7.691254315868693, 'latency_in_seconds': 0.1300178044999484}\n",
      "CPU times: total: 3h 51min 20s\n",
      "Wall time: 41min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Without quantization \n",
    "data = load_dataset(\"imdb\", split=\"test\").shuffle(seed=42).select(range(1000))\n",
    "task_evaluator = evaluator('sentiment-analysis')\n",
    "results = []\n",
    "\n",
    "for model_id in models:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=2,\n",
    "    )\n",
    "    eval_results = task_evaluator.compute(\n",
    "        model_or_pipeline=pipeline('sentiment-analysis', model=model, tokenizer=tokenizer),\n",
    "        data=data,\n",
    "        label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "        metric=evaluate.combine([\"accuracy\", \"recall\", \"precision\", \"f1\"]),\n",
    "    )\n",
    "    print(f'{model_id=} {eval_results=}')\n",
    "    results.append(eval_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T04:18:46.672115Z",
     "start_time": "2024-03-28T03:37:05.133534Z"
    }
   },
   "id": "b389d05095cc5562",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'PeftModelForSequenceClassification' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/bert-base-uncased' eval_results={'accuracy': 0.923, 'recall': 0.9262295081967213, 'precision': 0.9168356997971603, 'f1': 0.9215086646279307, 'total_time_in_seconds': 261.37462470005266, 'samples_per_second': 3.8259261056714147, 'latency_in_seconds': 0.2613746247000526}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'PeftModelForSequenceClassification' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/bert-large-uncased' eval_results={'accuracy': 0.923, 'recall': 0.9200819672131147, 'precision': 0.9219712525667351, 'f1': 0.921025641025641, 'total_time_in_seconds': 904.6657828000607, 'samples_per_second': 1.1053805935987402, 'latency_in_seconds': 0.9046657828000606}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'PeftModelForSequenceClassification' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/roberta-base' eval_results={'accuracy': 0.937, 'recall': 0.9098360655737705, 'precision': 0.958963282937365, 'f1': 0.9337539432176656, 'total_time_in_seconds': 252.01753239997197, 'samples_per_second': 3.9679779040647065, 'latency_in_seconds': 0.25201753239997193}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'PeftModelForSequenceClassification' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/roberta-large' eval_results={'accuracy': 0.965, 'recall': 0.9487704918032787, 'precision': 0.9788583509513742, 'f1': 0.963579604578564, 'total_time_in_seconds': 871.5159360000398, 'samples_per_second': 1.1474259490763394, 'latency_in_seconds': 0.8715159360000398}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The model 'PeftModelForSequenceClassification' is not supported for sentiment-analysis. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id='peft/distilbert-base-uncased' eval_results={'accuracy': 0.862, 'recall': 0.8463114754098361, 'precision': 0.8676470588235294, 'f1': 0.8568464730290456, 'total_time_in_seconds': 143.15038430003915, 'samples_per_second': 6.9856606036350435, 'latency_in_seconds': 0.14315038430003915}\n",
      "CPU times: total: 3h 49min 23s\n",
      "Wall time: 41min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# With quantization\n",
    "quantization_results = []\n",
    "\n",
    "for model_id in models:\n",
    "    peft_config = PeftConfig.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    base_model = AutoModelForSequenceClassification.from_pretrained(model_id.split('/')[1])\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model=base_model,\n",
    "        model_id=model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        num_labels=2,\n",
    "    )\n",
    "    eval_results = task_evaluator.compute(\n",
    "        model_or_pipeline=pipeline('sentiment-analysis', model=model, tokenizer=tokenizer),\n",
    "        data=data,\n",
    "        label_mapping={\"LABEL_0\": 0, \"LABEL_1\": 1},\n",
    "        metric=evaluate.combine([\"accuracy\", \"recall\", \"precision\", \"f1\"]),\n",
    "    )\n",
    "    print(f'{model_id=} {eval_results=}')\n",
    "    quantization_results.append(eval_results)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T05:01:14.677804Z",
     "start_time": "2024-03-28T04:20:09.240811Z"
    }
   },
   "id": "846441d2c6d4c578",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                              accuracy    recall  precision        f1  \\\npeft/bert-base-uncased           0.921  0.926230   0.913131  0.919634   \npeft/bert-large-uncased          0.923  0.920082   0.921971  0.921026   \npeft/roberta-base                0.937  0.913934   0.955032  0.934031   \npeft/roberta-large               0.965  0.948770   0.978858  0.963580   \npeft/distilbert-base-uncased     0.861  0.844262   0.867368  0.855659   \n\n                              total_time_in_seconds  samples_per_second  \\\npeft/bert-base-uncased                   263.371240            3.796922   \npeft/bert-large-uncased                  910.154669            1.098714   \npeft/roberta-base                        259.067528            3.859997   \npeft/roberta-large                       885.460542            1.129356   \npeft/distilbert-base-uncased             130.017804            7.691254   \n\n                              latency_in_seconds  \npeft/bert-base-uncased                  0.263371  \npeft/bert-large-uncased                 0.910155  \npeft/roberta-base                       0.259068  \npeft/roberta-large                      0.885461  \npeft/distilbert-base-uncased            0.130018  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>f1</th>\n      <th>total_time_in_seconds</th>\n      <th>samples_per_second</th>\n      <th>latency_in_seconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>peft/bert-base-uncased</th>\n      <td>0.921</td>\n      <td>0.926230</td>\n      <td>0.913131</td>\n      <td>0.919634</td>\n      <td>263.371240</td>\n      <td>3.796922</td>\n      <td>0.263371</td>\n    </tr>\n    <tr>\n      <th>peft/bert-large-uncased</th>\n      <td>0.923</td>\n      <td>0.920082</td>\n      <td>0.921971</td>\n      <td>0.921026</td>\n      <td>910.154669</td>\n      <td>1.098714</td>\n      <td>0.910155</td>\n    </tr>\n    <tr>\n      <th>peft/roberta-base</th>\n      <td>0.937</td>\n      <td>0.913934</td>\n      <td>0.955032</td>\n      <td>0.934031</td>\n      <td>259.067528</td>\n      <td>3.859997</td>\n      <td>0.259068</td>\n    </tr>\n    <tr>\n      <th>peft/roberta-large</th>\n      <td>0.965</td>\n      <td>0.948770</td>\n      <td>0.978858</td>\n      <td>0.963580</td>\n      <td>885.460542</td>\n      <td>1.129356</td>\n      <td>0.885461</td>\n    </tr>\n    <tr>\n      <th>peft/distilbert-base-uncased</th>\n      <td>0.861</td>\n      <td>0.844262</td>\n      <td>0.867368</td>\n      <td>0.855659</td>\n      <td>130.017804</td>\n      <td>7.691254</td>\n      <td>0.130018</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results, index=models)\n",
    "df[[\"accuracy\", \"recall\", \"precision\", \"f1\", \"total_time_in_seconds\", 'samples_per_second', 'latency_in_seconds']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T05:02:00.990280Z",
     "start_time": "2024-03-28T05:02:00.976276Z"
    }
   },
   "id": "1b604aaa74795471",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                              accuracy    recall  precision        f1  \\\npeft/bert-base-uncased           0.923  0.926230   0.916836  0.921509   \npeft/bert-large-uncased          0.923  0.920082   0.921971  0.921026   \npeft/roberta-base                0.937  0.909836   0.958963  0.933754   \npeft/roberta-large               0.965  0.948770   0.978858  0.963580   \npeft/distilbert-base-uncased     0.862  0.846311   0.867647  0.856846   \n\n                              total_time_in_seconds  samples_per_second  \\\npeft/bert-base-uncased                   261.374625            3.825926   \npeft/bert-large-uncased                  904.665783            1.105381   \npeft/roberta-base                        252.017532            3.967978   \npeft/roberta-large                       871.515936            1.147426   \npeft/distilbert-base-uncased             143.150384            6.985661   \n\n                              latency_in_seconds  \npeft/bert-base-uncased                  0.261375  \npeft/bert-large-uncased                 0.904666  \npeft/roberta-base                       0.252018  \npeft/roberta-large                      0.871516  \npeft/distilbert-base-uncased            0.143150  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>accuracy</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>f1</th>\n      <th>total_time_in_seconds</th>\n      <th>samples_per_second</th>\n      <th>latency_in_seconds</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>peft/bert-base-uncased</th>\n      <td>0.923</td>\n      <td>0.926230</td>\n      <td>0.916836</td>\n      <td>0.921509</td>\n      <td>261.374625</td>\n      <td>3.825926</td>\n      <td>0.261375</td>\n    </tr>\n    <tr>\n      <th>peft/bert-large-uncased</th>\n      <td>0.923</td>\n      <td>0.920082</td>\n      <td>0.921971</td>\n      <td>0.921026</td>\n      <td>904.665783</td>\n      <td>1.105381</td>\n      <td>0.904666</td>\n    </tr>\n    <tr>\n      <th>peft/roberta-base</th>\n      <td>0.937</td>\n      <td>0.909836</td>\n      <td>0.958963</td>\n      <td>0.933754</td>\n      <td>252.017532</td>\n      <td>3.967978</td>\n      <td>0.252018</td>\n    </tr>\n    <tr>\n      <th>peft/roberta-large</th>\n      <td>0.965</td>\n      <td>0.948770</td>\n      <td>0.978858</td>\n      <td>0.963580</td>\n      <td>871.515936</td>\n      <td>1.147426</td>\n      <td>0.871516</td>\n    </tr>\n    <tr>\n      <th>peft/distilbert-base-uncased</th>\n      <td>0.862</td>\n      <td>0.846311</td>\n      <td>0.867647</td>\n      <td>0.856846</td>\n      <td>143.150384</td>\n      <td>6.985661</td>\n      <td>0.143150</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfq = pd.DataFrame(quantization_results, index=models)\n",
    "dfq[[\"accuracy\", \"recall\", \"precision\", \"f1\", \"total_time_in_seconds\", 'samples_per_second', 'latency_in_seconds']]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T05:02:01.509924Z",
     "start_time": "2024-03-28T05:02:01.500922Z"
    }
   },
   "id": "f25d94d51d415adc",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9e1818633ced24e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
